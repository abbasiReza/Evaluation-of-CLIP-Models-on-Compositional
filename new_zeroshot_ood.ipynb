{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accordion',\n",
       " 'acorn',\n",
       " 'afghan hound',\n",
       " 'african chameleon',\n",
       " 'ambulance',\n",
       " 'american egret',\n",
       " 'american lobster',\n",
       " 'anemone fish',\n",
       " 'axolotl',\n",
       " 'baboon',\n",
       " 'backpack',\n",
       " 'badger',\n",
       " 'bagel',\n",
       " 'bald eagle',\n",
       " 'balloon',\n",
       " 'ballplayer',\n",
       " 'ballpoint',\n",
       " 'banana',\n",
       " 'bannister',\n",
       " 'barbell',\n",
       " 'barbershop',\n",
       " 'barn',\n",
       " 'barometer',\n",
       " 'barrel',\n",
       " 'barrow',\n",
       " 'basketball',\n",
       " 'basset',\n",
       " 'bath towel',\n",
       " 'bathtub',\n",
       " 'beacon',\n",
       " 'beagle',\n",
       " 'beaver',\n",
       " 'bee',\n",
       " 'beer bottle',\n",
       " 'beer glass',\n",
       " 'binder',\n",
       " 'binoculars',\n",
       " 'birdhouse',\n",
       " 'bison',\n",
       " 'black swan',\n",
       " 'bloodhound',\n",
       " 'bookcase',\n",
       " 'border collie',\n",
       " 'boston bull',\n",
       " 'bow tie',\n",
       " 'boxer',\n",
       " 'broccoli',\n",
       " 'broom',\n",
       " 'bucket',\n",
       " 'bulletproof vest',\n",
       " 'burrito',\n",
       " 'caldron',\n",
       " 'candle',\n",
       " 'cannon',\n",
       " 'canoe',\n",
       " 'cardigan',\n",
       " 'carousel',\n",
       " 'carton',\n",
       " 'castle',\n",
       " 'cellular telephone',\n",
       " 'centipede',\n",
       " 'chain',\n",
       " 'cheeseburger',\n",
       " 'cheetah',\n",
       " 'chihuahua',\n",
       " 'chimpanzee',\n",
       " 'chow',\n",
       " 'cleaver',\n",
       " 'cocker spaniel',\n",
       " 'cockroach',\n",
       " 'collie',\n",
       " 'common iguana',\n",
       " 'common newt',\n",
       " 'corkscrew',\n",
       " 'cowboy hat',\n",
       " 'cricket',\n",
       " 'cucumber',\n",
       " 'dalmatian',\n",
       " 'dragonfly',\n",
       " 'drake',\n",
       " 'electric guitar',\n",
       " 'espresso',\n",
       " 'fire engine',\n",
       " 'flamingo',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'forklift',\n",
       " 'fox squirrel',\n",
       " 'french bulldog',\n",
       " 'gasmask',\n",
       " 'gazelle',\n",
       " 'german shepherd',\n",
       " 'giant panda',\n",
       " 'gibbon',\n",
       " 'goblet',\n",
       " 'golden retriever',\n",
       " 'goldfinch',\n",
       " 'goldfish',\n",
       " 'goose',\n",
       " 'grand piano',\n",
       " 'granny smith',\n",
       " 'grasshopper',\n",
       " 'grey whale',\n",
       " 'guillotine',\n",
       " 'guinea pig',\n",
       " 'hammer',\n",
       " 'hammerhead',\n",
       " 'harmonica',\n",
       " 'harp',\n",
       " 'hatchet',\n",
       " 'hermit crab',\n",
       " 'hippopotamus',\n",
       " 'hog',\n",
       " 'hotdog',\n",
       " 'hourglass',\n",
       " 'hummingbird',\n",
       " 'hyena',\n",
       " 'ice cream',\n",
       " 'indian cobra',\n",
       " 'ipod',\n",
       " 'italian greyhound',\n",
       " 'jeep',\n",
       " 'jellyfish',\n",
       " 'jersey',\n",
       " 'joystick',\n",
       " 'junco',\n",
       " 'killer whale',\n",
       " 'koala',\n",
       " 'lab coat',\n",
       " 'labrador retriever',\n",
       " 'ladle',\n",
       " 'ladybug',\n",
       " 'lampshade',\n",
       " 'lawn mower',\n",
       " 'lemon',\n",
       " 'leopard',\n",
       " 'letter opener',\n",
       " 'lifeboat',\n",
       " 'lighter',\n",
       " 'lion',\n",
       " 'lipstick',\n",
       " 'llama',\n",
       " 'lorikeet',\n",
       " 'magnetic compass',\n",
       " 'mailbox',\n",
       " 'mantis',\n",
       " 'matchstick',\n",
       " 'meerkat',\n",
       " 'milk can',\n",
       " 'minivan',\n",
       " 'missile',\n",
       " 'mitten',\n",
       " 'monarch',\n",
       " 'mushroom',\n",
       " 'notebook',\n",
       " 'ocarina',\n",
       " 'orangutan',\n",
       " 'ostrich',\n",
       " 'paddle',\n",
       " 'paintbrush',\n",
       " 'parachute',\n",
       " 'peacock',\n",
       " 'pelican',\n",
       " 'pembroke',\n",
       " 'pencil box',\n",
       " 'pickup',\n",
       " 'pineapple',\n",
       " 'pinwheel',\n",
       " 'pirate',\n",
       " 'pizza',\n",
       " 'plow',\n",
       " 'pomegranate',\n",
       " 'pomeranian',\n",
       " 'porcupine',\n",
       " 'pretzel',\n",
       " 'puffer',\n",
       " 'pug',\n",
       " 'radiator',\n",
       " 'radio',\n",
       " 'revolver',\n",
       " 'rottweiler',\n",
       " 'rugby ball',\n",
       " 'saint bernard',\n",
       " 'sandal',\n",
       " 'sax',\n",
       " 'scabbard',\n",
       " 'school bus',\n",
       " 'schooner',\n",
       " 'scorpion',\n",
       " 'scotch terrier',\n",
       " 'scuba diver',\n",
       " 'shield',\n",
       " 'shih-tzu',\n",
       " 'siberian husky',\n",
       " 'skunk',\n",
       " 'snail',\n",
       " 'snow leopard',\n",
       " 'soccer ball',\n",
       " 'sock',\n",
       " 'space shuttle',\n",
       " 'spider web',\n",
       " 'spoonbill',\n",
       " 'standard poodle',\n",
       " 'starfish',\n",
       " 'steam locomotive',\n",
       " 'stingray',\n",
       " 'stole',\n",
       " 'stove',\n",
       " 'strawberry',\n",
       " 'submarine',\n",
       " 'sundial',\n",
       " 'sunglass',\n",
       " 'swimming cap',\n",
       " 'tabby',\n",
       " 'tank',\n",
       " 'tarantula',\n",
       " 'tennis ball',\n",
       " 'tiger',\n",
       " 'timber wolf',\n",
       " 'toucan',\n",
       " 'toy poodle',\n",
       " 'tractor',\n",
       " 'tree frog',\n",
       " 'trombone',\n",
       " 'vase',\n",
       " 'violin',\n",
       " 'volcano',\n",
       " 'vulture',\n",
       " 'warplane',\n",
       " 'weimaraner',\n",
       " 'west highland white terrier',\n",
       " 'whippet',\n",
       " 'wine bottle',\n",
       " 'yorkshire terrier',\n",
       " 'zebra']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import open_clip\n",
    "texts = os.listdir('E:/only_objs')\n",
    "texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_IMAGENET_TEMPLATES = (\n",
    "    lambda c: f'a bad photo of a {c}.',\n",
    "    lambda c: f'a photo of many {c}.',\n",
    "    lambda c: f'a sculpture of a {c}.',\n",
    "    lambda c: f'a photo of the hard to see {c}.',\n",
    "    lambda c: f'a low resolution photo of the {c}.',\n",
    "    lambda c: f'a rendering of a {c}.',\n",
    "    lambda c: f'graffiti of a {c}.',\n",
    "    lambda c: f'a bad photo of the {c}.',\n",
    "    lambda c: f'a cropped photo of the {c}.',\n",
    "    lambda c: f'a tattoo of a {c}.',\n",
    "    lambda c: f'the embroidered {c}.',\n",
    "    lambda c: f'a photo of a hard to see {c}.',\n",
    "    lambda c: f'a bright photo of a {c}.',\n",
    "    lambda c: f'a photo of a clean {c}.',\n",
    "    lambda c: f'a photo of a dirty {c}.',\n",
    "    lambda c: f'a dark photo of the {c}.',\n",
    "    lambda c: f'a drawing of a {c}.',\n",
    "    lambda c: f'a photo of my {c}.',\n",
    "    lambda c: f'the plastic {c}.',\n",
    "    lambda c: f'a photo of the cool {c}.',\n",
    "    lambda c: f'a close-up photo of a {c}.',\n",
    "    lambda c: f'a black and white photo of the {c}.',\n",
    "    lambda c: f'a painting of the {c}.',\n",
    "    lambda c: f'a painting of a {c}.',\n",
    "    lambda c: f'a pixelated photo of the {c}.',\n",
    "    lambda c: f'a sculpture of the {c}.',\n",
    "    lambda c: f'a bright photo of the {c}.',\n",
    "    lambda c: f'a cropped photo of a {c}.',\n",
    "    lambda c: f'a plastic {c}.',\n",
    "    lambda c: f'a photo of the dirty {c}.',\n",
    "    lambda c: f'a jpeg corrupted photo of a {c}.',\n",
    "    lambda c: f'a blurry photo of the {c}.',\n",
    "    lambda c: f'a photo of the {c}.',\n",
    "    lambda c: f'a good photo of the {c}.',\n",
    "    lambda c: f'a rendering of the {c}.',\n",
    "    lambda c: f'a {c} in a video game.',\n",
    "    lambda c: f'a photo of one {c}.',\n",
    "    lambda c: f'a doodle of a {c}.',\n",
    "    lambda c: f'a close-up photo of the {c}.',\n",
    "    lambda c: f'a photo of a {c}.',\n",
    "    lambda c: f'the origami {c}.',\n",
    "    lambda c: f'the {c} in a video game.',\n",
    "    lambda c: f'a sketch of a {c}.',\n",
    "    lambda c: f'a doodle of the {c}.',\n",
    "    lambda c: f'a origami {c}.',\n",
    "    lambda c: f'a low resolution photo of a {c}.',\n",
    "    lambda c: f'the toy {c}.',\n",
    "    lambda c: f'a rendition of the {c}.',\n",
    "    lambda c: f'a photo of the clean {c}.',\n",
    "    lambda c: f'a photo of a large {c}.',\n",
    "    lambda c: f'a rendition of a {c}.',\n",
    "    lambda c: f'a photo of a nice {c}.',\n",
    "    lambda c: f'a photo of a weird {c}.',\n",
    "    lambda c: f'a blurry photo of a {c}.',\n",
    "    lambda c: f'a cartoon {c}.',\n",
    "    lambda c: f'art of a {c}.',\n",
    "    lambda c: f'a sketch of the {c}.',\n",
    "    lambda c: f'a embroidered {c}.',\n",
    "    lambda c: f'a pixelated photo of a {c}.',\n",
    "    lambda c: f'itap of the {c}.',\n",
    "    lambda c: f'a jpeg corrupted photo of the {c}.',\n",
    "    lambda c: f'a good photo of a {c}.',\n",
    "    lambda c: f'a plushie {c}.',\n",
    "    lambda c: f'a photo of the nice {c}.',\n",
    "    lambda c: f'a photo of the small {c}.',\n",
    "    lambda c: f'a photo of the weird {c}.',\n",
    "    lambda c: f'the cartoon {c}.',\n",
    "    lambda c: f'art of the {c}.',\n",
    "    lambda c: f'a drawing of the {c}.',\n",
    "    lambda c: f'a photo of the large {c}.',\n",
    "    lambda c: f'a black and white photo of a {c}.',\n",
    "    lambda c: f'the plushie {c}.',\n",
    "    lambda c: f'a dark photo of a {c}.',\n",
    "    lambda c: f'itap of a {c}.',\n",
    "    lambda c: f'graffiti of the {c}.',\n",
    "    lambda c: f'a toy {c}.',\n",
    "    lambda c: f'itap of my {c}.',\n",
    "    lambda c: f'a photo of a cool {c}.',\n",
    "    lambda c: f'a photo of a small {c}.',\n",
    "    lambda c: f'a tattoo of the {c}.',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts=[]\n",
    "texts=sorted(texts)\n",
    "for text in texts:\n",
    "    for a in OPENAI_IMAGENET_TEMPLATES:\n",
    "        # print(a(text))\n",
    "        all_texts.append(a(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "all_models=open_clip.list_pretrained()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    # print(pred)\n",
    "    # print(target)\n",
    "\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    # print(correct)\n",
    "    # print([float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk])\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>in_dis_Acc_top1</th>\n",
       "      <th>in_dis_Acc_top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, in_dis_Acc_top1, in_dis_Acc_top5]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_in=pd.DataFrame(columns=['model','in_dis_Acc_top1','in_dis_Acc_top5'])\n",
    "df_in"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT-bigG-14', 'laion2b_s39b_b160k'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'coca_ViT-L-14_mscoco_finetuned_laion2b_s13b_b90k', 'in_dis_Acc_top1': 0.3287803828780383, 'in_dis_Acc_top5': 0.4668745966874597}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autocast = torch.cuda.amp.autocast\n",
    "cast_dtype = None\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='commonpool_xl_s13b_b90k')\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "dataset=torchvision.datasets.ImageFolder('E:/final_dataset/',transform=preprocess)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,shuffle=False,batch_size=64\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "# Example list of texts to encode\n",
    "\n",
    "with torch.no_grad():\n",
    "# Specify batch size\n",
    "    batch_size = 80\n",
    "\n",
    "    # Encode texts in batches\n",
    "    text_features_target = []\n",
    "    for i in range(0, len(all_texts), batch_size):\n",
    "        batch_texts = all_texts[i:i+batch_size]\n",
    "        text = tokenizer(batch_texts)\n",
    "        with torch.no_grad():\n",
    "            text_features_batch = F.normalize(model.encode_text(text.cuda()))\n",
    "            # text.cuda()\n",
    "            # print(text_features_batch.shape)\n",
    "            text_features_batch=text_features_batch.mean(dim=0)\n",
    "\n",
    "            # text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "            text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # print(text_features_batch.unsqueeze(0).shape)\n",
    "            # print(text_features_batch.mean(dim=0).shape)\n",
    "            text_features_target.append(text_features_batch.unsqueeze(0))\n",
    "\n",
    "    # Concatenate text features from batches\n",
    "    text_features_target = torch.cat(text_features_target, dim=0)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for images, target in dataloader:\n",
    "        images = images.to('cuda')\n",
    "        if cast_dtype is not None:\n",
    "            images = images.to(dtype=cast_dtype)\n",
    "        target = target.to('cuda')\n",
    "\n",
    "\n",
    "        with autocast():\n",
    "            # predict\n",
    "            image_features = model.encode_image(images)\n",
    "            # image_features = F.normalize(image_features, dim=-1)\n",
    "            logits = 100. * image_features @ text_features_target.T\n",
    "        # print(image_features.shape)\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        # print(acc1, acc5)\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "    \n",
    "top1 = (top1 / n)\n",
    "top5 = (top5 / n)\n",
    "# row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':top1,'in_dis_Acc_top5':top5}\n",
    "print(row)\n",
    "# df_in = df_in.append(row, ignore_index=True)\n",
    "# df_in.to_csv('all_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5734566573456658\n",
      "0.8778231877823188\n"
     ]
    }
   ],
   "source": [
    "print(top1)\n",
    "print(top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "autocast = torch.cuda.amp.autocast\n",
    "cast_dtype = None\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='commonpool_xl_s13b_b90k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder('E:/final_dataset/',transform=preprocess)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=64\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "# Example list of texts to encode\n",
    "\n",
    "with torch.no_grad():\n",
    "# Specify batch size\n",
    "    batch_size = 80\n",
    "\n",
    "    # Encode texts in batches\n",
    "    text_features_target = []\n",
    "    for i in range(0, len(all_texts), batch_size):\n",
    "        batch_texts = all_texts[i:i+batch_size]\n",
    "        text = tokenizer(batch_texts)\n",
    "        with torch.no_grad():\n",
    "            text_features_batch = F.normalize(model.encode_text(text.cuda()))\n",
    "            # text.cuda()\n",
    "            # print(text_features_batch.shape)\n",
    "            text_features_batch=text_features_batch.mean(dim=0)\n",
    "\n",
    "            # text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "            text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # print(text_features_batch.unsqueeze(0).shape)\n",
    "            # print(text_features_batch.mean(dim=0).shape)\n",
    "            text_features_target.append(text_features_batch.unsqueeze(0))\n",
    "\n",
    "    # Concatenate text features from batches\n",
    "    text_features_target = torch.cat(text_features_target, dim=0)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for images, target in dataloader:\n",
    "        images = images.to('cuda')\n",
    "        if cast_dtype is not None:\n",
    "            images = images.to(dtype=cast_dtype)\n",
    "        target = target.to('cuda')\n",
    "\n",
    "\n",
    "        with autocast():\n",
    "            # predict\n",
    "            image_features = model.encode_image(images)\n",
    "            # image_features = F.normalize(image_features, dim=-1)\n",
    "            logits = 100. * image_features @ text_features_target.T\n",
    "        # print(image_features.shape)\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        # print(acc1, acc5)\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "    \n",
    "top1 = (top1 / n)\n",
    "top5 = (top5 / n)\n",
    "# row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':top1,'in_dis_Acc_top5':top5}\n",
    "# print(row)\n",
    "# df_in = df_in.append(row, ignore_index=True)\n",
    "# df_in.to_csv('all_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021510002151000217\n",
      "0.010217251021725103\n"
     ]
    }
   ],
   "source": [
    "print(top1)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('coca_ViT-B-32', 'laion2b_s13b_b90k')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models[76]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'coca_ViT-B-32_laion2b_s13b_b90k', 'in_dis_Acc_top1': 0.2762959776295978, 'in_dis_Acc_top5': 0.43611529361152934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_16256\\3099959336.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'coca_ViT-B-32_mscoco_finetuned_laion2b_s13b_b90k', 'in_dis_Acc_top1': 0.001505700150570015, 'in_dis_Acc_top5': 0.009141750914175092}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_16256\\3099959336.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'coca_ViT-L-14_laion2b_s13b_b90k', 'in_dis_Acc_top1': 0.3217896321789632, 'in_dis_Acc_top5': 0.4585932458593246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_16256\\3099959336.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'coca_ViT-L-14_mscoco_finetuned_laion2b_s13b_b90k', 'in_dis_Acc_top1': 0.3287803828780383, 'in_dis_Acc_top5': 0.4668745966874597}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_16256\\3099959336.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autocast = torch.cuda.amp.autocast\n",
    "cast_dtype = None\n",
    "for i,a in enumerate(all_models):\n",
    "    # try:\n",
    "        if i<76:\n",
    "            continue\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(a[0], pretrained=a[1])\n",
    "        tokenizer = open_clip.get_tokenizer(a[0])\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        dataset=torchvision.datasets.ImageFolder('E:/final_dataset',transform=preprocess)\n",
    "        dataloader=torch.utils.data.DataLoader(dataset,shuffle=False,batch_size=64\n",
    "                                        )\n",
    "        \n",
    "\n",
    "\n",
    "        # Example list of texts to encode\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        # Specify batch size\n",
    "            batch_size = 80\n",
    "\n",
    "            # Encode texts in batches\n",
    "            text_features_target = []\n",
    "            for i in range(0, len(all_texts), batch_size):\n",
    "                batch_texts = all_texts[i:i+batch_size]\n",
    "                text = tokenizer(batch_texts)\n",
    "                with torch.no_grad():\n",
    "                    text_features_batch = F.normalize(model.encode_text(text.cuda()))\n",
    "                    # text.cuda()\n",
    "                    # print(text_features_batch.shape)\n",
    "                    text_features_batch=text_features_batch.mean(dim=0)\n",
    "\n",
    "                    # text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "                    text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                    # print(text_features_batch.unsqueeze(0).shape)\n",
    "                    # print(text_features_batch.mean(dim=0).shape)\n",
    "                    text_features_target.append(text_features_batch.unsqueeze(0))\n",
    "\n",
    "            # Concatenate text features from batches\n",
    "            text_features_target = torch.cat(text_features_target, dim=0)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            top1, top5, n = 0., 0., 0.\n",
    "            for images, target in dataloader:\n",
    "                images = images.to('cuda')\n",
    "                if cast_dtype is not None:\n",
    "                    images = images.to(dtype=cast_dtype)\n",
    "                target = target.to('cuda')\n",
    "\n",
    "\n",
    "                with autocast():\n",
    "                    # predict\n",
    "                    image_features = model.encode_image(images)\n",
    "                    # image_features = F.normalize(image_features, dim=-1)\n",
    "                    logits = 100. * image_features @ text_features_target.T\n",
    "                # print(image_features.shape)\n",
    "                # measure accuracy\n",
    "                acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "                # print(acc1, acc5)\n",
    "                top1 += acc1\n",
    "                top5 += acc5\n",
    "                n += images.size(0)\n",
    "            \n",
    "        top1 = (top1 / n)\n",
    "        top5 = (top5 / n)\n",
    "        row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':top1,'in_dis_Acc_top5':top5}\n",
    "        print(row)\n",
    "        df_in = df_in.append(row, ignore_index=True)\n",
    "        df_in.to_csv('all_model_ood_new_.csv', index=False)\n",
    "\n",
    "    # except:\n",
    "    #     row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':0,'in_dis_Acc_top5':0}\n",
    "    #     print(row)\n",
    "    #     df_in = df_in.append(row, ignore_index=True)\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r 'E:/final_dataset/striped ballplayer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510d8ac1378f461db27b89cfb80fdb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user01\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4087eb3915784a0b94b6fbdc09374cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f05da6369f846979ac2f55eb5cb7503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open_clip\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('xlm-roberta-base-ViT-B-32', pretrained='laion5b_s13b_b90k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtimm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m k\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(all_models[k][\u001b[39m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m model, _, preprocess \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mcreate_model_and_transforms(all_models[k][\u001b[39m0\u001b[39m], pretrained\u001b[39m=\u001b[39mall_models[k][\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import timm\n",
    "k=80\n",
    "print(all_models[k][0])\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(all_models[k][0], pretrained=all_models[k][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (0.4.9)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from timm) (0.14.1+cu117)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from timm) (1.13.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torch>=1.4->timm) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torchvision->timm) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('convnext_base', 'laion400m_s13b_b51k')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:338: UserWarning: C:\\Users\\user01/.cache/clip\\vit_l_14-laion400m_e32-3d133497.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "  5%|█▉                                  | 90.0M/1.71G [10:45<3:13:41, 139kiB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, _, preprocess \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39;49mcreate_model_and_transforms(\u001b[39m'\u001b[39;49m\u001b[39mViT-L-14\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlaion400m_e32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\factory.py:292\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, image_mean, image_std, aug_cfg, cache_dir, output_dict)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    275\u001b[0m         model_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    276\u001b[0m         pretrained: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m         output_dict: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    291\u001b[0m ):\n\u001b[1;32m--> 292\u001b[0m     model \u001b[39m=\u001b[39m create_model(\n\u001b[0;32m    293\u001b[0m         model_name,\n\u001b[0;32m    294\u001b[0m         pretrained,\n\u001b[0;32m    295\u001b[0m         precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[0;32m    296\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    297\u001b[0m         jit\u001b[39m=\u001b[39;49mjit,\n\u001b[0;32m    298\u001b[0m         force_quick_gelu\u001b[39m=\u001b[39;49mforce_quick_gelu,\n\u001b[0;32m    299\u001b[0m         force_custom_text\u001b[39m=\u001b[39;49mforce_custom_text,\n\u001b[0;32m    300\u001b[0m         force_patch_dropout\u001b[39m=\u001b[39;49mforce_patch_dropout,\n\u001b[0;32m    301\u001b[0m         force_image_size\u001b[39m=\u001b[39;49mforce_image_size,\n\u001b[0;32m    302\u001b[0m         pretrained_image\u001b[39m=\u001b[39;49mpretrained_image,\n\u001b[0;32m    303\u001b[0m         pretrained_hf\u001b[39m=\u001b[39;49mpretrained_hf,\n\u001b[0;32m    304\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    305\u001b[0m         output_dict\u001b[39m=\u001b[39;49moutput_dict,\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    308\u001b[0m     image_mean \u001b[39m=\u001b[39m image_mean \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(model\u001b[39m.\u001b[39mvisual, \u001b[39m'\u001b[39m\u001b[39mimage_mean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    309\u001b[0m     image_std \u001b[39m=\u001b[39m image_std \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(model\u001b[39m.\u001b[39mvisual, \u001b[39m'\u001b[39m\u001b[39mimage_std\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\factory.py:201\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained)\u001b[0m\n\u001b[0;32m    199\u001b[0m pretrained_cfg \u001b[39m=\u001b[39m get_pretrained_cfg(model_name, pretrained)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m pretrained_cfg:\n\u001b[1;32m--> 201\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m download_pretrained(pretrained_cfg, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[0;32m    202\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(pretrained):\n\u001b[0;32m    203\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m pretrained\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:393\u001b[0m, in \u001b[0;36mdownload_pretrained\u001b[1;34m(cfg, force_hf_hub, cache_dir)\u001b[0m\n\u001b[0;32m    390\u001b[0m     download_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m download_url:\n\u001b[1;32m--> 393\u001b[0m     target \u001b[39m=\u001b[39m download_pretrained_from_url(download_url, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[0;32m    394\u001b[0m \u001b[39melif\u001b[39;00m download_hf_hub:\n\u001b[0;32m    395\u001b[0m     has_hf_hub(\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:345\u001b[0m, in \u001b[0;36mdownload_pretrained_from_url\u001b[1;34m(url, cache_dir)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(source\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Length\u001b[39m\u001b[39m\"\u001b[39m)), ncols\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39miB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m loop:\n\u001b[0;32m    344\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m         buffer \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mread(\u001b[39m8192\u001b[39;49m)\n\u001b[0;32m    346\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buffer:\n\u001b[0;32m    347\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: accordion, Accuracy: 0.7544\n",
      "Class: acorn, Accuracy: 0.7833\n",
      "Class: afghan hound, Accuracy: 0.9231\n",
      "Class: african chameleon, Accuracy: 1.0000\n",
      "Class: ambulance, Accuracy: 0.7500\n",
      "Class: american egret, Accuracy: 1.0000\n",
      "Class: american lobster, Accuracy: 1.0000\n",
      "Class: anemone fish, Accuracy: 0.7500\n",
      "Class: axolotl, Accuracy: 1.0000\n",
      "Class: baboon, Accuracy: 0.9565\n",
      "Class: backpack, Accuracy: 0.9667\n",
      "Class: badger, Accuracy: 1.0000\n",
      "Class: bagel, Accuracy: 1.0000\n",
      "Class: bald eagle, Accuracy: 0.9500\n",
      "Class: balloon, Accuracy: 0.6250\n",
      "Class: ballplayer, Accuracy: 0.2059\n",
      "Class: ballpoint, Accuracy: 0.8194\n",
      "Class: banana, Accuracy: 0.6957\n",
      "Class: bannister, Accuracy: 0.7544\n",
      "Class: barbell, Accuracy: 0.9375\n",
      "Class: barbershop, Accuracy: 0.8551\n",
      "Class: barn, Accuracy: 1.0000\n",
      "Class: barometer, Accuracy: 0.6702\n",
      "Class: barrel, Accuracy: 0.8182\n",
      "Class: barrow, Accuracy: 0.8167\n",
      "Class: basketball, Accuracy: 0.9062\n",
      "Class: basset, Accuracy: 0.9750\n",
      "Class: bath towel, Accuracy: 0.2500\n",
      "Class: bathtub, Accuracy: 0.8636\n",
      "Class: beacon, Accuracy: 0.4615\n",
      "Class: beagle, Accuracy: 0.7667\n",
      "Class: beaver, Accuracy: 0.8750\n",
      "Class: bee, Accuracy: 0.5000\n",
      "Class: beer bottle, Accuracy: 0.9000\n",
      "Class: beer glass, Accuracy: 0.9583\n",
      "Class: binder, Accuracy: 0.5882\n",
      "Class: binoculars, Accuracy: 0.7875\n",
      "Class: birdhouse, Accuracy: 0.9531\n",
      "Class: bison, Accuracy: 0.9375\n",
      "Class: black swan, Accuracy: 1.0000\n",
      "Class: bloodhound, Accuracy: 0.9250\n",
      "Class: bookcase, Accuracy: 0.8158\n",
      "Class: border collie, Accuracy: 0.6250\n",
      "Class: boston bull, Accuracy: 0.6071\n",
      "Class: bow tie, Accuracy: 1.0000\n",
      "Class: boxer, Accuracy: 0.7500\n",
      "Class: broccoli, Accuracy: 1.0000\n",
      "Class: broom, Accuracy: 0.6667\n",
      "Class: bucket, Accuracy: 0.7429\n",
      "Class: bulletproof vest, Accuracy: 0.9545\n",
      "Class: burrito, Accuracy: 0.7778\n",
      "Class: caldron, Accuracy: 0.9737\n",
      "Class: candle, Accuracy: 1.0000\n",
      "Class: cannon, Accuracy: 0.6410\n",
      "Class: canoe, Accuracy: 0.5957\n",
      "Class: cardigan, Accuracy: 0.9545\n",
      "Class: carousel, Accuracy: 0.7500\n",
      "Class: carton, Accuracy: 0.0208\n",
      "Class: castle, Accuracy: 0.9375\n",
      "Class: cellular telephone, Accuracy: 0.7857\n",
      "Class: centipede, Accuracy: 0.9706\n",
      "Class: chain, Accuracy: 0.7500\n",
      "Class: cheeseburger, Accuracy: 1.0000\n",
      "Class: cheetah, Accuracy: 0.7778\n",
      "Class: chihuahua, Accuracy: 1.0000\n",
      "Class: chimpanzee, Accuracy: 0.9375\n",
      "Class: chow, Accuracy: 0.9032\n",
      "Class: cleaver, Accuracy: 0.7925\n",
      "Class: cocker spaniel, Accuracy: 1.0000\n",
      "Class: cockroach, Accuracy: 0.9583\n",
      "Class: collie, Accuracy: 1.0000\n",
      "Class: common iguana, Accuracy: 1.0000\n",
      "Class: common newt, Accuracy: 1.0000\n",
      "Class: corkscrew, Accuracy: 0.7532\n",
      "Class: cowboy hat, Accuracy: 0.9722\n",
      "Class: cricket, Accuracy: 0.4839\n",
      "Class: cucumber, Accuracy: 0.8909\n",
      "Class: dalmatian, Accuracy: 1.0000\n",
      "Class: dragonfly, Accuracy: 0.8846\n",
      "Class: drake, Accuracy: 0.5625\n",
      "Class: electric guitar, Accuracy: 1.0000\n",
      "Class: espresso, Accuracy: 0.9167\n",
      "Class: fire engine, Accuracy: 0.7500\n",
      "Class: flamingo, Accuracy: 1.0000\n",
      "Class: flute, Accuracy: 0.7500\n",
      "Class: fly, Accuracy: 1.0000\n",
      "Class: forklift, Accuracy: 0.9231\n",
      "Class: fox squirrel, Accuracy: 1.0000\n",
      "Class: french bulldog, Accuracy: 1.0000\n",
      "Class: gasmask, Accuracy: 0.9867\n",
      "Class: gazelle, Accuracy: 0.9180\n",
      "Class: german shepherd, Accuracy: 1.0000\n",
      "Class: giant panda, Accuracy: 1.0000\n",
      "Class: gibbon, Accuracy: 0.9091\n",
      "Class: goblet, Accuracy: 0.8939\n",
      "Class: golden retriever, Accuracy: 1.0000\n",
      "Class: goldfinch, Accuracy: 1.0000\n",
      "Class: goldfish, Accuracy: 0.9643\n",
      "Class: goose, Accuracy: 0.9167\n",
      "Class: grand piano, Accuracy: 0.8750\n",
      "Class: granny smith, Accuracy: 1.0000\n",
      "Class: grasshopper, Accuracy: 0.7500\n",
      "Class: grey whale, Accuracy: 1.0000\n",
      "Class: guillotine, Accuracy: 0.6500\n",
      "Class: guinea pig, Accuracy: 0.9545\n",
      "Class: hammer, Accuracy: 0.6562\n",
      "Class: hammerhead, Accuracy: 1.0000\n",
      "Class: harmonica, Accuracy: 0.4118\n",
      "Class: harp, Accuracy: 0.8000\n",
      "Class: hatchet, Accuracy: 0.5781\n",
      "Class: hermit crab, Accuracy: 1.0000\n",
      "Class: hippopotamus, Accuracy: 1.0000\n",
      "Class: hog, Accuracy: 1.0000\n",
      "Class: hotdog, Accuracy: 0.8696\n",
      "Class: hourglass, Accuracy: 1.0000\n",
      "Class: hummingbird, Accuracy: 1.0000\n",
      "Class: hyena, Accuracy: 0.9643\n",
      "Class: ice cream, Accuracy: 1.0000\n",
      "Class: indian cobra, Accuracy: 1.0000\n",
      "Class: ipod, Accuracy: 0.6667\n",
      "Class: italian greyhound, Accuracy: 0.9583\n",
      "Class: jeep, Accuracy: 0.9615\n",
      "Class: jellyfish, Accuracy: 1.0000\n",
      "Class: jersey, Accuracy: 0.5000\n",
      "Class: joystick, Accuracy: 0.8966\n",
      "Class: junco, Accuracy: 1.0000\n",
      "Class: killer whale, Accuracy: 1.0000\n",
      "Class: koala, Accuracy: 1.0000\n",
      "Class: lab coat, Accuracy: 1.0000\n",
      "Class: labrador retriever, Accuracy: 0.7250\n",
      "Class: ladle, Accuracy: 0.6818\n",
      "Class: ladybug, Accuracy: 1.0000\n",
      "Class: lampshade, Accuracy: 0.9355\n",
      "Class: lawn mower, Accuracy: 0.8750\n",
      "Class: lemon, Accuracy: 0.7812\n",
      "Class: leopard, Accuracy: 0.8929\n",
      "Class: letter opener, Accuracy: 0.0000\n",
      "Class: lifeboat, Accuracy: 0.7527\n",
      "Class: lighter, Accuracy: 0.9091\n",
      "Class: lion, Accuracy: 1.0000\n",
      "Class: lipstick, Accuracy: 0.9286\n",
      "Class: llama, Accuracy: 1.0000\n",
      "Class: lorikeet, Accuracy: 1.0000\n",
      "Class: magnetic compass, Accuracy: 0.9167\n",
      "Class: mailbox, Accuracy: 0.9155\n",
      "Class: mantis, Accuracy: 1.0000\n",
      "Class: matchstick, Accuracy: 0.9773\n",
      "Class: meerkat, Accuracy: 1.0000\n",
      "Class: milk can, Accuracy: 0.7857\n",
      "Class: minivan, Accuracy: 0.9529\n",
      "Class: missile, Accuracy: 0.6667\n",
      "Class: mitten, Accuracy: 0.7273\n",
      "Class: monarch, Accuracy: 0.7955\n",
      "Class: mushroom, Accuracy: 0.8571\n",
      "Class: notebook, Accuracy: 0.6923\n",
      "Class: ocarina, Accuracy: 0.5568\n",
      "Class: orangutan, Accuracy: 1.0000\n",
      "Class: ostrich, Accuracy: 0.9512\n",
      "Class: paddle, Accuracy: 0.4211\n",
      "Class: paintbrush, Accuracy: 0.7736\n",
      "Class: parachute, Accuracy: 0.5938\n",
      "Class: peacock, Accuracy: 0.9375\n",
      "Class: pelican, Accuracy: 0.9683\n",
      "Class: pembroke, Accuracy: 0.0000\n",
      "Class: pencil box, Accuracy: 1.0000\n",
      "Class: pickup, Accuracy: 0.7292\n",
      "Class: pineapple, Accuracy: 1.0000\n",
      "Class: pinwheel, Accuracy: 0.7719\n",
      "Class: pirate, Accuracy: 0.8519\n",
      "Class: pizza, Accuracy: 0.9375\n",
      "Class: plow, Accuracy: 0.3846\n",
      "Class: pomegranate, Accuracy: 0.9531\n",
      "Class: pomeranian, Accuracy: 0.9322\n",
      "Class: porcupine, Accuracy: 1.0000\n",
      "Class: pretzel, Accuracy: 0.8077\n",
      "Class: puffer, Accuracy: 0.9722\n",
      "Class: pug, Accuracy: 1.0000\n",
      "Class: radiator, Accuracy: 0.7736\n",
      "Class: radio, Accuracy: 1.0000\n",
      "Class: revolver, Accuracy: 1.0000\n",
      "Class: rottweiler, Accuracy: 1.0000\n",
      "Class: rugby ball, Accuracy: 0.8750\n",
      "Class: saint bernard, Accuracy: 0.9000\n",
      "Class: sandal, Accuracy: 0.8333\n",
      "Class: sax, Accuracy: 0.9375\n",
      "Class: scabbard, Accuracy: 0.3800\n",
      "Class: school bus, Accuracy: 1.0000\n",
      "Class: schooner, Accuracy: 0.7262\n",
      "Class: scorpion, Accuracy: 0.8846\n",
      "Class: scotch terrier, Accuracy: 1.0000\n",
      "Class: scuba diver, Accuracy: 0.8125\n",
      "Class: shield, Accuracy: 0.9583\n",
      "Class: shih-tzu, Accuracy: 0.9733\n",
      "Class: siberian husky, Accuracy: 0.9583\n",
      "Class: skunk, Accuracy: 0.8596\n",
      "Class: snail, Accuracy: 1.0000\n",
      "Class: snow leopard, Accuracy: 1.0000\n",
      "Class: soccer ball, Accuracy: 1.0000\n",
      "Class: sock, Accuracy: 1.0000\n",
      "Class: space shuttle, Accuracy: 0.2500\n",
      "Class: spider web, Accuracy: 0.0000\n",
      "Class: spoonbill, Accuracy: 0.9286\n",
      "Class: standard poodle, Accuracy: 1.0000\n",
      "Class: starfish, Accuracy: 1.0000\n",
      "Class: steam locomotive, Accuracy: 1.0000\n",
      "Class: stingray, Accuracy: 0.8947\n",
      "Class: stole, Accuracy: 0.0500\n",
      "Class: stove, Accuracy: 0.7097\n",
      "Class: strawberry, Accuracy: 1.0000\n",
      "Class: submarine, Accuracy: 0.6875\n",
      "Class: sundial, Accuracy: 0.4792\n",
      "Class: sunglass, Accuracy: 1.0000\n",
      "Class: swimming cap, Accuracy: 1.0000\n",
      "Class: tabby, Accuracy: 0.9167\n",
      "Class: tank, Accuracy: 0.6667\n",
      "Class: tarantula, Accuracy: 1.0000\n",
      "Class: tennis ball, Accuracy: 1.0000\n",
      "Class: tiger, Accuracy: 0.7778\n",
      "Class: timber wolf, Accuracy: 1.0000\n",
      "Class: toucan, Accuracy: 0.9773\n",
      "Class: toy poodle, Accuracy: 1.0000\n",
      "Class: tractor, Accuracy: 0.9062\n",
      "Class: tree frog, Accuracy: 1.0000\n",
      "Class: trombone, Accuracy: 0.8125\n",
      "Class: vase, Accuracy: 0.7045\n",
      "Class: violin, Accuracy: 0.9605\n",
      "Class: volcano, Accuracy: 0.9444\n",
      "Class: vulture, Accuracy: 1.0000\n",
      "Class: warplane, Accuracy: 0.8382\n",
      "Class: weimaraner, Accuracy: 0.9516\n",
      "Class: west highland white terrier, Accuracy: 1.0000\n",
      "Class: whippet, Accuracy: 0.1552\n",
      "Class: wine bottle, Accuracy: 0.8750\n",
      "Class: yorkshire terrier, Accuracy: 0.9828\n",
      "Class: zebra, Accuracy: 0.8148\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store class-wise accuracy\n",
    "class_accuracy = {}\n",
    "autocast = torch.cuda.amp.autocast\n",
    "cast_dtype = None\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder('E:/only_objs/',transform=preprocess)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=64\n",
    "                                )\n",
    "# Example list of texts to encode\n",
    "\n",
    "with torch.no_grad():\n",
    "# Specify batch size\n",
    "    batch_size = 80\n",
    "\n",
    "    # Encode texts in batches\n",
    "    text_features_target = []\n",
    "    for i in range(0, len(all_texts), batch_size):\n",
    "        batch_texts = all_texts[i:i+batch_size]\n",
    "        text = tokenizer(batch_texts)\n",
    "        with torch.no_grad():\n",
    "            text_features_batch = F.normalize(model.encode_text(text.cuda()))\n",
    "            # text.cuda()\n",
    "            # print(text_features_batch.shape)\n",
    "            text_features_batch=text_features_batch.mean(dim=0)\n",
    "\n",
    "            # text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "            text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # print(text_features_batch.unsqueeze(0).shape)\n",
    "            # print(text_features_batch.mean(dim=0).shape)\n",
    "            text_features_target.append(text_features_batch.unsqueeze(0))\n",
    "\n",
    "    # Concatenate text features from batches\n",
    "    text_features_target = torch.cat(text_features_target, dim=0)\n",
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    class_counts = torch.zeros(len(dataset.classes))  # Track the number of samples per class\n",
    "    class_corrects = torch.zeros(len(dataset.classes))  # Track the number of correct predictions per class\n",
    "    \n",
    "    for images, target in dataloader:\n",
    "        images = images.to('cuda')\n",
    "        if cast_dtype is not None:\n",
    "            images = images.to(dtype=cast_dtype)\n",
    "        target = target.to('cuda')\n",
    "\n",
    "        with autocast():\n",
    "            # Predict\n",
    "            image_features = model.encode_image(images)\n",
    "            logits = 100. * image_features @ text_features_target.T\n",
    "        \n",
    "        # Measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        \n",
    "        # Update class-wise accuracy\n",
    "        _, predicted = logits.max(1)\n",
    "        for pred, true in zip(predicted, target):\n",
    "            class_counts[true] += 1\n",
    "            if pred == true:\n",
    "                class_corrects[true] += 1\n",
    "        \n",
    "        # Update top-1 and top-5 accuracy\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "    \n",
    "    # Calculate overall top-1 and top-5 accuracy\n",
    "    top1 = top1 / n\n",
    "    top5 = top5 / n\n",
    "    \n",
    "    # Calculate class-wise accuracy\n",
    "    for i, class_name in enumerate(dataset.classes):\n",
    "        class_accuracy[class_name] = class_corrects[i] / class_counts[i]\n",
    "\n",
    "# Print class-wise accuracy\n",
    "for class_name, accuracy in class_accuracy.items():\n",
    "    print(f\"Class: {class_name}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5479673047967305"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
