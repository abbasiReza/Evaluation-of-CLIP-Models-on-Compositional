{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accordion',\n",
       " 'afghan hound',\n",
       " 'african chameleon',\n",
       " 'ambulance',\n",
       " 'american egret',\n",
       " 'american lobster',\n",
       " 'anemone fish',\n",
       " 'ashcan',\n",
       " 'axolotl',\n",
       " 'baboon',\n",
       " 'backpack',\n",
       " 'badger',\n",
       " 'bagel',\n",
       " 'bald eagle',\n",
       " 'balloon',\n",
       " 'ballpoint',\n",
       " 'banana',\n",
       " 'bannister',\n",
       " 'barbell',\n",
       " 'barbershop',\n",
       " 'barn',\n",
       " 'barometer',\n",
       " 'barrel',\n",
       " 'barrow',\n",
       " 'basketball',\n",
       " 'basset',\n",
       " 'bath towel',\n",
       " 'bathtub',\n",
       " 'beacon',\n",
       " 'beagle',\n",
       " 'beaver',\n",
       " 'bee',\n",
       " 'beer bottle',\n",
       " 'beer glass',\n",
       " 'binder',\n",
       " 'binoculars',\n",
       " 'birdhouse',\n",
       " 'bison',\n",
       " 'black swan',\n",
       " 'bloodhound',\n",
       " 'bookcase',\n",
       " 'border collie',\n",
       " 'boston bull',\n",
       " 'bow tie',\n",
       " 'boxer',\n",
       " 'broccoli',\n",
       " 'broom',\n",
       " 'bucket',\n",
       " 'bulletproof vest',\n",
       " 'burrito',\n",
       " 'caldron',\n",
       " 'candle',\n",
       " 'cannon',\n",
       " 'canoe',\n",
       " 'cardigan',\n",
       " 'carousel',\n",
       " 'carton',\n",
       " 'castle',\n",
       " 'cellular telephone',\n",
       " 'centipede',\n",
       " 'chain',\n",
       " 'cheeseburger',\n",
       " 'cheetah',\n",
       " 'chihuahua',\n",
       " 'chimpanzee',\n",
       " 'chow',\n",
       " 'cleaver',\n",
       " 'cocker spaniel',\n",
       " 'cockroach',\n",
       " 'collie',\n",
       " 'common iguana',\n",
       " 'common newt',\n",
       " 'corkscrew',\n",
       " 'cowboy hat',\n",
       " 'cricket',\n",
       " 'cucumber',\n",
       " 'dalmatian',\n",
       " 'dragonfly',\n",
       " 'drake',\n",
       " 'electric guitar',\n",
       " 'espresso',\n",
       " 'fire engine',\n",
       " 'flamingo',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'forklift',\n",
       " 'fox squirrel',\n",
       " 'french bulldog',\n",
       " 'gasmask',\n",
       " 'gazelle',\n",
       " 'german shepherd',\n",
       " 'giant panda',\n",
       " 'gibbon',\n",
       " 'goblet',\n",
       " 'golden retriever',\n",
       " 'goldfinch',\n",
       " 'goldfish',\n",
       " 'goose',\n",
       " 'grand piano',\n",
       " 'granny smith',\n",
       " 'grasshopper',\n",
       " 'grey whale',\n",
       " 'guillotine',\n",
       " 'guinea pig',\n",
       " 'hammer',\n",
       " 'hammerhead',\n",
       " 'harmonica',\n",
       " 'harp',\n",
       " 'hatchet',\n",
       " 'hermit crab',\n",
       " 'hippopotamus',\n",
       " 'hog',\n",
       " 'hotdog',\n",
       " 'hourglass',\n",
       " 'hummingbird',\n",
       " 'hyena',\n",
       " 'ice cream',\n",
       " 'indian cobra',\n",
       " 'ipod',\n",
       " 'italian greyhound',\n",
       " 'jeep',\n",
       " 'jellyfish',\n",
       " 'jersey',\n",
       " 'joystick',\n",
       " 'junco',\n",
       " 'killer whale',\n",
       " 'koala',\n",
       " 'lab coat',\n",
       " 'labrador retriever',\n",
       " 'ladle',\n",
       " 'ladybug',\n",
       " 'lampshade',\n",
       " 'lawn mower',\n",
       " 'lemon',\n",
       " 'leopard',\n",
       " 'letter opener',\n",
       " 'lifeboat',\n",
       " 'lighter',\n",
       " 'lion',\n",
       " 'lipstick',\n",
       " 'llama',\n",
       " 'lorikeet',\n",
       " 'magnetic compass',\n",
       " 'mailbox',\n",
       " 'mantis',\n",
       " 'matchstick',\n",
       " 'meerkat',\n",
       " 'milk can',\n",
       " 'minivan',\n",
       " 'missile',\n",
       " 'mitten',\n",
       " 'monarch',\n",
       " 'mushroom',\n",
       " 'notebook',\n",
       " 'ocarina',\n",
       " 'orangutan',\n",
       " 'ostrich',\n",
       " 'paddle',\n",
       " 'paintbrush',\n",
       " 'parachute',\n",
       " 'peacock',\n",
       " 'pelican',\n",
       " 'pembroke',\n",
       " 'pencil box',\n",
       " 'pickup',\n",
       " 'pineapple',\n",
       " 'pinwheel',\n",
       " 'pirate',\n",
       " 'pizza',\n",
       " 'plow',\n",
       " 'pomegranate',\n",
       " 'pomeranian',\n",
       " 'porcupine',\n",
       " 'pretzel',\n",
       " 'puffer',\n",
       " 'pug',\n",
       " 'radiator',\n",
       " 'radio',\n",
       " 'revolver',\n",
       " 'rottweiler',\n",
       " 'rugby ball',\n",
       " 'saint bernard',\n",
       " 'sandal',\n",
       " 'sax',\n",
       " 'scabbard',\n",
       " 'school bus',\n",
       " 'schooner',\n",
       " 'scorpion',\n",
       " 'scotch terrier',\n",
       " 'shield',\n",
       " 'shih-tzu',\n",
       " 'siberian husky',\n",
       " 'skunk',\n",
       " 'snail',\n",
       " 'snow leopard',\n",
       " 'soccer ball',\n",
       " 'sock',\n",
       " 'space shuttle',\n",
       " 'spider web',\n",
       " 'spoonbill',\n",
       " 'standard poodle',\n",
       " 'starfish',\n",
       " 'steam locomotive',\n",
       " 'stingray',\n",
       " 'stole',\n",
       " 'stove',\n",
       " 'strawberry',\n",
       " 'submarine',\n",
       " 'sundial',\n",
       " 'sunglass',\n",
       " 'swimming cap',\n",
       " 'tabby',\n",
       " 'tank',\n",
       " 'tarantula',\n",
       " 'tennis ball',\n",
       " 'tiger',\n",
       " 'timber wolf',\n",
       " 'toucan',\n",
       " 'toy poodle',\n",
       " 'tractor',\n",
       " 'tree frog',\n",
       " 'trombone',\n",
       " 'vase',\n",
       " 'violin',\n",
       " 'volcano',\n",
       " 'vulture',\n",
       " 'warplane',\n",
       " 'weimaraner',\n",
       " 'west highland white terrier',\n",
       " 'whippet',\n",
       " 'wine bottle',\n",
       " 'yorkshire terrier',\n",
       " 'zebra']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import open_clip\n",
    "texts = os.listdir('E:/in-dis -with-name')\n",
    "texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_IMAGENET_TEMPLATES = (\n",
    "    lambda c: f'a bad photo of a {c}.',\n",
    "    lambda c: f'a photo of many {c}.',\n",
    "    lambda c: f'a sculpture of a {c}.',\n",
    "    lambda c: f'a photo of the hard to see {c}.',\n",
    "    lambda c: f'a low resolution photo of the {c}.',\n",
    "    lambda c: f'a rendering of a {c}.',\n",
    "    lambda c: f'graffiti of a {c}.',\n",
    "    lambda c: f'a bad photo of the {c}.',\n",
    "    lambda c: f'a cropped photo of the {c}.',\n",
    "    lambda c: f'a tattoo of a {c}.',\n",
    "    lambda c: f'the embroidered {c}.',\n",
    "    lambda c: f'a photo of a hard to see {c}.',\n",
    "    lambda c: f'a bright photo of a {c}.',\n",
    "    lambda c: f'a photo of a clean {c}.',\n",
    "    lambda c: f'a photo of a dirty {c}.',\n",
    "    lambda c: f'a dark photo of the {c}.',\n",
    "    lambda c: f'a drawing of a {c}.',\n",
    "    lambda c: f'a photo of my {c}.',\n",
    "    lambda c: f'the plastic {c}.',\n",
    "    lambda c: f'a photo of the cool {c}.',\n",
    "    lambda c: f'a close-up photo of a {c}.',\n",
    "    lambda c: f'a black and white photo of the {c}.',\n",
    "    lambda c: f'a painting of the {c}.',\n",
    "    lambda c: f'a painting of a {c}.',\n",
    "    lambda c: f'a pixelated photo of the {c}.',\n",
    "    lambda c: f'a sculpture of the {c}.',\n",
    "    lambda c: f'a bright photo of the {c}.',\n",
    "    lambda c: f'a cropped photo of a {c}.',\n",
    "    lambda c: f'a plastic {c}.',\n",
    "    lambda c: f'a photo of the dirty {c}.',\n",
    "    lambda c: f'a jpeg corrupted photo of a {c}.',\n",
    "    lambda c: f'a blurry photo of the {c}.',\n",
    "    lambda c: f'a photo of the {c}.',\n",
    "    lambda c: f'a good photo of the {c}.',\n",
    "    lambda c: f'a rendering of the {c}.',\n",
    "    lambda c: f'a {c} in a video game.',\n",
    "    lambda c: f'a photo of one {c}.',\n",
    "    lambda c: f'a doodle of a {c}.',\n",
    "    lambda c: f'a close-up photo of the {c}.',\n",
    "    lambda c: f'a photo of a {c}.',\n",
    "    lambda c: f'the origami {c}.',\n",
    "    lambda c: f'the {c} in a video game.',\n",
    "    lambda c: f'a sketch of a {c}.',\n",
    "    lambda c: f'a doodle of the {c}.',\n",
    "    lambda c: f'a origami {c}.',\n",
    "    lambda c: f'a low resolution photo of a {c}.',\n",
    "    lambda c: f'the toy {c}.',\n",
    "    lambda c: f'a rendition of the {c}.',\n",
    "    lambda c: f'a photo of the clean {c}.',\n",
    "    lambda c: f'a photo of a large {c}.',\n",
    "    lambda c: f'a rendition of a {c}.',\n",
    "    lambda c: f'a photo of a nice {c}.',\n",
    "    lambda c: f'a photo of a weird {c}.',\n",
    "    lambda c: f'a blurry photo of a {c}.',\n",
    "    lambda c: f'a cartoon {c}.',\n",
    "    lambda c: f'art of a {c}.',\n",
    "    lambda c: f'a sketch of the {c}.',\n",
    "    lambda c: f'a embroidered {c}.',\n",
    "    lambda c: f'a pixelated photo of a {c}.',\n",
    "    lambda c: f'itap of the {c}.',\n",
    "    lambda c: f'a jpeg corrupted photo of the {c}.',\n",
    "    lambda c: f'a good photo of a {c}.',\n",
    "    lambda c: f'a plushie {c}.',\n",
    "    lambda c: f'a photo of the nice {c}.',\n",
    "    lambda c: f'a photo of the small {c}.',\n",
    "    lambda c: f'a photo of the weird {c}.',\n",
    "    lambda c: f'the cartoon {c}.',\n",
    "    lambda c: f'art of the {c}.',\n",
    "    lambda c: f'a drawing of the {c}.',\n",
    "    lambda c: f'a photo of the large {c}.',\n",
    "    lambda c: f'a black and white photo of a {c}.',\n",
    "    lambda c: f'the plushie {c}.',\n",
    "    lambda c: f'a dark photo of a {c}.',\n",
    "    lambda c: f'itap of a {c}.',\n",
    "    lambda c: f'graffiti of the {c}.',\n",
    "    lambda c: f'a toy {c}.',\n",
    "    lambda c: f'itap of my {c}.',\n",
    "    lambda c: f'a photo of a cool {c}.',\n",
    "    lambda c: f'a photo of a small {c}.',\n",
    "    lambda c: f'a tattoo of the {c}.',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts=[]\n",
    "for text in texts:\n",
    "    for a in OPENAI_IMAGENET_TEMPLATES:\n",
    "        # print(a(text))\n",
    "        all_texts.append(a(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "all_models=open_clip.list_pretrained()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    # print(pred)\n",
    "    # print(target)\n",
    "\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    # print(correct)\n",
    "    # print([float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk])\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>in_dis_Acc_top1</th>\n",
       "      <th>in_dis_Acc_top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, in_dis_Acc_top1, in_dis_Acc_top5]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_in=pd.DataFrame(columns=['model','in_dis_Acc_top1','in_dis_Acc_top5'])\n",
    "df_in"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT-bigG-14', 'laion2b_s39b_b160k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m top1 \u001b[39m=\u001b[39m (top1 \u001b[39m/\u001b[39m n)\n\u001b[0;32m     63\u001b[0m top5 \u001b[39m=\u001b[39m (top5 \u001b[39m/\u001b[39m n)\n\u001b[1;32m---> 64\u001b[0m row\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m:a[\u001b[39m0\u001b[39;49m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39ma[\u001b[39m1\u001b[39m],\u001b[39m'\u001b[39m\u001b[39min_dis_Acc_top1\u001b[39m\u001b[39m'\u001b[39m:top1,\u001b[39m'\u001b[39m\u001b[39min_dis_Acc_top5\u001b[39m\u001b[39m'\u001b[39m:top5}\n\u001b[0;32m     65\u001b[0m \u001b[39mprint\u001b[39m(row)\n\u001b[0;32m     66\u001b[0m \u001b[39m# df_in = df_in.append(row, ignore_index=True)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# df_in.to_csv('all_model.csv', index=False)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "autocast = torch.cuda.amp.autocast\n",
    "cast_dtype = None\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-B-32', pretrained='mscoco_finetuned_laion2b_s13b_b90k')\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-B-32')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "dataset=torchvision.datasets.ImageFolder('E:/in-dis -with-name',transform=preprocess)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=64\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "# Example list of texts to encode\n",
    "\n",
    "with torch.no_grad():\n",
    "# Specify batch size\n",
    "    batch_size = 80\n",
    "\n",
    "    # Encode texts in batches\n",
    "    text_features_target = []\n",
    "    for i in range(0, len(all_texts), batch_size):\n",
    "        batch_texts = all_texts[i:i+batch_size]\n",
    "        text = tokenizer(batch_texts)\n",
    "        with torch.no_grad():\n",
    "            text_features_batch = F.normalize(model.encode_text(text.cuda()))\n",
    "            # text.cuda()\n",
    "            # print(text_features_batch.shape)\n",
    "            text_features_batch=text_features_batch.mean(dim=0)\n",
    "\n",
    "            # text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "            text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # print(text_features_batch.unsqueeze(0).shape)\n",
    "            # print(text_features_batch.mean(dim=0).shape)\n",
    "            text_features_target.append(text_features_batch.unsqueeze(0))\n",
    "\n",
    "    # Concatenate text features from batches\n",
    "    text_features_target = torch.cat(text_features_target, dim=0)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for images, target in dataloader:\n",
    "        images = images.to('cuda')\n",
    "        if cast_dtype is not None:\n",
    "            images = images.to(dtype=cast_dtype)\n",
    "        target = target.to('cuda')\n",
    "\n",
    "\n",
    "        with autocast():\n",
    "            # predict\n",
    "            image_features = model.encode_image(images)\n",
    "            # image_features = F.normalize(image_features, dim=-1)\n",
    "            logits = 100. * image_features @ text_features_target.T\n",
    "        # print(image_features.shape)\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        # print(acc1, acc5)\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "    \n",
    "top1 = (top1 / n)\n",
    "top5 = (top5 / n)\n",
    "row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':top1,'in_dis_Acc_top5':top5}\n",
    "print(row)\n",
    "# df_in = df_in.append(row, ignore_index=True)\n",
    "# df_in.to_csv('all_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028101025773640202 0.09223342815274545\n"
     ]
    }
   ],
   "source": [
    "print(top1,top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT-H-14_laion2b_s32b_b79k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:338: UserWarning: C:\\Users\\user01/.cache/clip\\vit_l_14-laion400m_e32-3d133497.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "  0%|                                    | 3.02M/1.71G [00:11<1:51:28, 255kiB/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-L-14_laion400m_e32', 'in_dis_Acc_top1': 0, 'in_dis_Acc_top5': 0}\n",
      "{'model': 'ViT-L-14_laion2b_s32b_b82k', 'in_dis_Acc_top1': 0.8932850616326179, 'in_dis_Acc_top5': 0.9755193517800189}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb75e55e4c94fa198e7c449b53ccd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-L-14_datacomp_xl_s13b_b90k', 'in_dis_Acc_top1': 0.9140591328333765, 'in_dis_Acc_top5': 0.9834497026118438}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab9bc57f1c64fc182ced2061aecd171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-L-14_commonpool_xl_clip_s13b_b90k', 'in_dis_Acc_top1': 0.9020774071200759, 'in_dis_Acc_top5': 0.9800879234548746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d99f6abae74b17a6be2ffcb4bf0f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-L-14_commonpool_xl_laion_s13b_b90k', 'in_dis_Acc_top1': 0.8903542798034653, 'in_dis_Acc_top5': 0.9753469528488924}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9801298da07740718a1e760df3afd598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-L-14_commonpool_xl_s13b_b90k', 'in_dis_Acc_top1': 0.892767864839238, 'in_dis_Acc_top5': 0.9777605378846651}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:70: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-L-14-336_openai', 'in_dis_Acc_top1': 0, 'in_dis_Acc_top5': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b586f9800d7d42a0a09b58c71e3ecb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-H-14_laion2b_s32b_b79k', 'in_dis_Acc_top1': 0, 'in_dis_Acc_top5': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b121bca4c01841cdaa6deb22a57b8dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/5.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ViT-g-14_laion2b_s12b_b42k', 'in_dis_Acc_top1': 0, 'in_dis_Acc_top5': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_6284\\1880225241.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_in = df_in.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autocast = torch.cuda.amp.autocast\n",
    "cast_dtype = None\n",
    "for i,a in enumerate(all_models):\n",
    "    try:\n",
    "        if i<50:\n",
    "            continue\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(a[0], pretrained=a[1])\n",
    "        tokenizer = open_clip.get_tokenizer(a[0])\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        dataset=torchvision.datasets.ImageFolder('E:/in-dis -with-name',transform=preprocess)\n",
    "        dataloader=torch.utils.data.DataLoader(dataset,shuffle=False,batch_size=64\n",
    "                                        )\n",
    "        \n",
    "\n",
    "\n",
    "        # Example list of texts to encode\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        # Specify batch size\n",
    "            batch_size = 80\n",
    "\n",
    "            # Encode texts in batches\n",
    "            text_features_target = []\n",
    "            for i in range(0, len(all_texts), batch_size):\n",
    "                batch_texts = all_texts[i:i+batch_size]\n",
    "                text = tokenizer(batch_texts)\n",
    "                with torch.no_grad():\n",
    "                    text_features_batch = F.normalize(model.encode_text(text.cuda()))\n",
    "                    # text.cuda()\n",
    "                    # print(text_features_batch.shape)\n",
    "                    text_features_batch=text_features_batch.mean(dim=0)\n",
    "\n",
    "                    # text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "                    text_features_batch /= text_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                    # print(text_features_batch.unsqueeze(0).shape)\n",
    "                    # print(text_features_batch.mean(dim=0).shape)\n",
    "                    text_features_target.append(text_features_batch.unsqueeze(0))\n",
    "\n",
    "            # Concatenate text features from batches\n",
    "            text_features_target = torch.cat(text_features_target, dim=0)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            top1, top5, n = 0., 0., 0.\n",
    "            for images, target in dataloader:\n",
    "                images = images.to('cuda')\n",
    "                if cast_dtype is not None:\n",
    "                    images = images.to(dtype=cast_dtype)\n",
    "                target = target.to('cuda')\n",
    "\n",
    "\n",
    "                with autocast():\n",
    "                    # predict\n",
    "                    image_features = model.encode_image(images)\n",
    "                    # image_features = F.normalize(image_features, dim=-1)\n",
    "                    logits = 100. * image_features @ text_features_target.T\n",
    "                # print(image_features.shape)\n",
    "                # measure accuracy\n",
    "                acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "                # print(acc1, acc5)\n",
    "                top1 += acc1\n",
    "                top5 += acc5\n",
    "                n += images.size(0)\n",
    "            \n",
    "        top1 = (top1 / n)\n",
    "        top5 = (top5 / n)\n",
    "        row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':top1,'in_dis_Acc_top5':top5}\n",
    "        print(row)\n",
    "        df_in = df_in.append(row, ignore_index=True)\n",
    "        df_in.to_csv('all_model_in_new5.csv', index=False)\n",
    "\n",
    "    except:\n",
    "        row={'model':a[0]+'_'+a[1],'in_dis_Acc_top1':0,'in_dis_Acc_top5':0}\n",
    "        print(row)\n",
    "        df_in = df_in.append(row, ignore_index=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510d8ac1378f461db27b89cfb80fdb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user01\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4087eb3915784a0b94b6fbdc09374cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f05da6369f846979ac2f55eb5cb7503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open_clip\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('xlm-roberta-base-ViT-B-32', pretrained='laion5b_s13b_b90k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtimm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m k\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(all_models[k][\u001b[39m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m model, _, preprocess \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mcreate_model_and_transforms(all_models[k][\u001b[39m0\u001b[39m], pretrained\u001b[39m=\u001b[39mall_models[k][\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import timm\n",
    "k=80\n",
    "print(all_models[k][0])\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(all_models[k][0], pretrained=all_models[k][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (0.4.9)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from timm) (0.14.1+cu117)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from timm) (1.13.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torch>=1.4->timm) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torchvision->timm) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages (from requests->torchvision->timm) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user01\\anaconda3\\envs\\bracs2\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('convnext_base', 'laion400m_s13b_b51k')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:338: UserWarning: C:\\Users\\user01/.cache/clip\\vit_l_14-laion400m_e32-3d133497.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "  5%|█▉                                  | 90.0M/1.71G [10:45<3:13:41, 139kiB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, _, preprocess \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39;49mcreate_model_and_transforms(\u001b[39m'\u001b[39;49m\u001b[39mViT-L-14\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlaion400m_e32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\factory.py:292\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, image_mean, image_std, aug_cfg, cache_dir, output_dict)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    275\u001b[0m         model_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    276\u001b[0m         pretrained: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m         output_dict: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    291\u001b[0m ):\n\u001b[1;32m--> 292\u001b[0m     model \u001b[39m=\u001b[39m create_model(\n\u001b[0;32m    293\u001b[0m         model_name,\n\u001b[0;32m    294\u001b[0m         pretrained,\n\u001b[0;32m    295\u001b[0m         precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[0;32m    296\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    297\u001b[0m         jit\u001b[39m=\u001b[39;49mjit,\n\u001b[0;32m    298\u001b[0m         force_quick_gelu\u001b[39m=\u001b[39;49mforce_quick_gelu,\n\u001b[0;32m    299\u001b[0m         force_custom_text\u001b[39m=\u001b[39;49mforce_custom_text,\n\u001b[0;32m    300\u001b[0m         force_patch_dropout\u001b[39m=\u001b[39;49mforce_patch_dropout,\n\u001b[0;32m    301\u001b[0m         force_image_size\u001b[39m=\u001b[39;49mforce_image_size,\n\u001b[0;32m    302\u001b[0m         pretrained_image\u001b[39m=\u001b[39;49mpretrained_image,\n\u001b[0;32m    303\u001b[0m         pretrained_hf\u001b[39m=\u001b[39;49mpretrained_hf,\n\u001b[0;32m    304\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    305\u001b[0m         output_dict\u001b[39m=\u001b[39;49moutput_dict,\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    308\u001b[0m     image_mean \u001b[39m=\u001b[39m image_mean \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(model\u001b[39m.\u001b[39mvisual, \u001b[39m'\u001b[39m\u001b[39mimage_mean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    309\u001b[0m     image_std \u001b[39m=\u001b[39m image_std \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(model\u001b[39m.\u001b[39mvisual, \u001b[39m'\u001b[39m\u001b[39mimage_std\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\factory.py:201\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained)\u001b[0m\n\u001b[0;32m    199\u001b[0m pretrained_cfg \u001b[39m=\u001b[39m get_pretrained_cfg(model_name, pretrained)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m pretrained_cfg:\n\u001b[1;32m--> 201\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m download_pretrained(pretrained_cfg, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[0;32m    202\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(pretrained):\n\u001b[0;32m    203\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m pretrained\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:393\u001b[0m, in \u001b[0;36mdownload_pretrained\u001b[1;34m(cfg, force_hf_hub, cache_dir)\u001b[0m\n\u001b[0;32m    390\u001b[0m     download_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m download_url:\n\u001b[1;32m--> 393\u001b[0m     target \u001b[39m=\u001b[39m download_pretrained_from_url(download_url, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[0;32m    394\u001b[0m \u001b[39melif\u001b[39;00m download_hf_hub:\n\u001b[0;32m    395\u001b[0m     has_hf_hub(\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:345\u001b[0m, in \u001b[0;36mdownload_pretrained_from_url\u001b[1;34m(url, cache_dir)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(source\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Length\u001b[39m\u001b[39m\"\u001b[39m)), ncols\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39miB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m loop:\n\u001b[0;32m    344\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m         buffer \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mread(\u001b[39m8192\u001b[39;49m)\n\u001b[0;32m    346\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buffer:\n\u001b[0;32m    347\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
