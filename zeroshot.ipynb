{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2389"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "texts = os.listdir('E:/final_dataset')\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:309: UserWarning: C:\\Users\\user01/.cache/clip\\vit_l_14-laion400m_e32-3d133497.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      " 77%|███████████████████████████▌        | 1.31G/1.71G [1:35:50<29:18, 228kiB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Model has been downloaded but the SHA256 checksum does not not match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopen_clip\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model, _, preprocess \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39;49mcreate_model_and_transforms(\u001b[39m'\u001b[39;49m\u001b[39mViT-L-14\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlaion400m_e32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mget_tokenizer(\u001b[39m'\u001b[39m\u001b[39mViT-L-14\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\factory.py:292\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, image_mean, image_std, aug_cfg, cache_dir, output_dict)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    275\u001b[0m         model_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    276\u001b[0m         pretrained: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m         output_dict: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    291\u001b[0m ):\n\u001b[1;32m--> 292\u001b[0m     model \u001b[39m=\u001b[39m create_model(\n\u001b[0;32m    293\u001b[0m         model_name,\n\u001b[0;32m    294\u001b[0m         pretrained,\n\u001b[0;32m    295\u001b[0m         precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[0;32m    296\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    297\u001b[0m         jit\u001b[39m=\u001b[39;49mjit,\n\u001b[0;32m    298\u001b[0m         force_quick_gelu\u001b[39m=\u001b[39;49mforce_quick_gelu,\n\u001b[0;32m    299\u001b[0m         force_custom_text\u001b[39m=\u001b[39;49mforce_custom_text,\n\u001b[0;32m    300\u001b[0m         force_patch_dropout\u001b[39m=\u001b[39;49mforce_patch_dropout,\n\u001b[0;32m    301\u001b[0m         force_image_size\u001b[39m=\u001b[39;49mforce_image_size,\n\u001b[0;32m    302\u001b[0m         pretrained_image\u001b[39m=\u001b[39;49mpretrained_image,\n\u001b[0;32m    303\u001b[0m         pretrained_hf\u001b[39m=\u001b[39;49mpretrained_hf,\n\u001b[0;32m    304\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    305\u001b[0m         output_dict\u001b[39m=\u001b[39;49moutput_dict,\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    308\u001b[0m     image_mean \u001b[39m=\u001b[39m image_mean \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(model\u001b[39m.\u001b[39mvisual, \u001b[39m'\u001b[39m\u001b[39mimage_mean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    309\u001b[0m     image_std \u001b[39m=\u001b[39m image_std \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(model\u001b[39m.\u001b[39mvisual, \u001b[39m'\u001b[39m\u001b[39mimage_std\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\factory.py:201\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained)\u001b[0m\n\u001b[0;32m    199\u001b[0m pretrained_cfg \u001b[39m=\u001b[39m get_pretrained_cfg(model_name, pretrained)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m pretrained_cfg:\n\u001b[1;32m--> 201\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m download_pretrained(pretrained_cfg, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[0;32m    202\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(pretrained):\n\u001b[0;32m    203\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m pretrained\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:364\u001b[0m, in \u001b[0;36mdownload_pretrained\u001b[1;34m(cfg, force_hf_hub, cache_dir)\u001b[0m\n\u001b[0;32m    361\u001b[0m     download_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[39mif\u001b[39;00m download_url:\n\u001b[1;32m--> 364\u001b[0m     target \u001b[39m=\u001b[39m download_pretrained_from_url(download_url, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[0;32m    365\u001b[0m \u001b[39melif\u001b[39;00m download_hf_hub:\n\u001b[0;32m    366\u001b[0m     has_hf_hub(\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\open_clip\\pretrained.py:324\u001b[0m, in \u001b[0;36mdownload_pretrained_from_url\u001b[1;34m(url, cache_dir)\u001b[0m\n\u001b[0;32m    321\u001b[0m             loop\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(buffer))\n\u001b[0;32m    323\u001b[0m \u001b[39mif\u001b[39;00m expected_sha256 \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m hashlib\u001b[39m.\u001b[39msha256(\u001b[39mopen\u001b[39m(download_target, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread())\u001b[39m.\u001b[39mhexdigest()\u001b[39m.\u001b[39mstartswith(expected_sha256):\n\u001b[1;32m--> 324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel has been downloaded but the SHA256 checksum does not not match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m download_target\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Model has been downloaded but the SHA256 checksum does not not match"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion400m_e32')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "model.cuda()\n",
    "# image = preprocess(Image.open(\"tomato2.jpeg\")).unsqueeze(0)\n",
    "text = tokenizer(texts)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    # image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text.cuda())\n",
    "    # image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # text_probs = (1.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2390, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for i,folder in enumerate(texts):\n",
    "    # print(i)\n",
    "    files=os.listdir('E:/final_dataset/'+folder)\n",
    "    for file in files:\n",
    "        image = preprocess(Image.open('E:/final_dataset/'+folder+'/'+file)).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image.cuda())\n",
    "            # text_features = model.encode_text(text.cuda())\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            # text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            # if max(text_probs[0])==text_probs[0][i]:\n",
    "            if torch.argmax(text_probs[0]).item() == i:\n",
    "                    counter+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5183831434100193\n"
     ]
    }
   ],
   "source": [
    "print(counter/9302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0695, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(text_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0695, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs[0][2232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1092e-09, 2.0575e-01, 1.2012e-08,  ..., 6.7666e-07, 1.6582e-07,\n",
       "        6.1352e-09], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c221ca3053d4da6bf2506facdec07b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10701226070122608\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='datacomp_m_s128m_b4k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "model.cuda()\n",
    "# image = preprocess(Image.open(\"tomato2.jpeg\")).unsqueeze(0)\n",
    "text = tokenizer(texts)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    # image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text.cuda())\n",
    "    # image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # text_probs = (1.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n",
    "\n",
    "counter=0\n",
    "total=0\n",
    "for i,folder in enumerate(texts):\n",
    "    # print(i)\n",
    "    files=os.listdir('E:/final_dataset/'+folder)\n",
    "    for file in files:\n",
    "        total+=1\n",
    "        image = preprocess(Image.open('E:/final_dataset/'+folder+'/'+file)).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image.cuda())\n",
    "            # text_features = model.encode_text(text.cuda())\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            # text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            # if max(text_probs[0])==text_probs[0][i]:\n",
    "            if torch.argmax(text_probs[0]).item() == i:\n",
    "            # if i in torch.topk(text_probs[0], 5).indices:\n",
    "                    counter+=1\n",
    "print(counter/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
